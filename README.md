# Data-Profiler-using-Spark
Data profiling application runs on Spark with Mongo dB as the database to extract and store the output of the profiler function. We have created this application in Spark that will read big datasets as input and will provide the profiling information about the dataset as an output. The goal of this project would be to build an API that would take a file as input stored in Mongo DB pointing to the big dataset and run a Spark job to infer all the statistics and then write the output back to a file (.csv or .txt) and store back in the database (Mongo DB). The idea is to make the profiling application available for not only a large variety of datasets but also to a large volume of datasets.
